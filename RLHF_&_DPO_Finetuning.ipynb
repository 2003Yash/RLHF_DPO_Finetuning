{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP5809NaWNatvEJLJZ0dqSe",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2003Yash/RLHF_DPO_Finetuning/blob/main/RLHF_%26_DPO_Finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Source: https://www.youtube.com/watch?v=bbVoDXoPrPM&list=PLZAGXXsIV3P3gCenOWRd56ZpeksdYFUKV"
      ],
      "metadata": {
        "id": "1M85UHhiPJrp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usually Model Creation Steps:\n",
        "\n",
        "- Pre-training (Creating the model)\n",
        "- SFT (Supervised Finetuning - Add extra knowledge)\n",
        "- RLHF (Make It behaviour more desired using human input)"
      ],
      "metadata": {
        "id": "uBSBcG_MPRLX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# How To Achieve RLHF:\n",
        "\n",
        "We make model make multiple output for each input and we rank them accordingly\n",
        "\n",
        "- only con of RLHF is it's restricted based on\n",
        "\n",
        "# DPO:\n",
        "\n",
        "- Instead of running PPO loops with rewards, rollouts, and all that gym-style chaos, DPO just takes pairs of answers (one preferred, one rejected) and directly nudges the model’s logits so it leans toward the preferred one. Think of it as teaching by comparison rather than teaching by scoring."
      ],
      "metadata": {
        "id": "N70MEvahPxCO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Below we will fine-tune an LLM to generate quality youtube video title based on video idea"
      ],
      "metadata": {
        "id": "PQYddtzmRlwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-1 (Create 5 Video Titles for Each Video Idea Each of 5 with 2 variations, and them manually choose each of 2 titlevaiation for all 5 titles and pic the best one manually)\n",
        "\n",
        "- Manual part is not mentioned in code, neeed to do it directly\n"
      ],
      "metadata": {
        "id": "0jKsHdtOQuT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import re\n",
        "from itertools import combinations\n",
        "\n",
        "from together import Together\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "# load vars from .env\n",
        "load_dotenv()\n",
        "\n",
        "# set together api key\n",
        "client = Together(api_key=os.getenv(\"TOGETHER_API_KEY\"))"
      ],
      "metadata": {
        "id": "u8zsJPKvRfE9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import ideas:\n",
        "\n",
        "# Open and read the CSV file\n",
        "with open('data/ideas.csv', mode='r', encoding='utf-8') as file:\n",
        "    reader = csv.reader(file)\n",
        "    next(reader) # skip first line\n",
        "\n",
        "    # intialize list to store ideas\n",
        "    idea_list = []\n",
        "\n",
        "    for row in reader:\n",
        "        idea_list.append(row[0])"
      ],
      "metadata": {
        "id": "4Y2t1ryNRiH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prompt template\n",
        "\n",
        "template = lambda idea : f\"\"\"**YouTube Titles**:\n",
        "- The 8 AI Skills That Will Separate Winners From Losers in 2025\n",
        "- World's Lightest Solid!\n",
        "- Why Are 96,000,000 Black Balls on This Reservoir?\n",
        "- These 11 income streams made me $220,000 in 2024.\n",
        "- I Make $15K/Month With 2 AI Apps\n",
        "- Top 5 Reasons Not to Become a Data Analyst\n",
        "- What Does a Data Analyst Actually Do?\n",
        "- How I Would Become a Data Analyst if I had to Start Over in 2024 | 6 Month Plan\n",
        "- How to learn to code FAST using ChatGPT (it's a game changer seriously)\n",
        "- 6 Years of Studying Machine Learning in 26 Minutes\n",
        "- My honest advice to someone who wants to be a data scientist\n",
        "- Complete Python Pandas Data Science Tutorial! (Reading CSV/Excel files, Sorting, Filtering, Groupby)\n",
        "- The Complete Machine Learning Roadmap\n",
        "- My GPT-evaluator got 1000% better with this simple trick.\n",
        "- The 5 paid subscriptions I actually use in 2025 as a Staff Software Engineer\n",
        "- AI Explained at 5 Levels of Complexity\n",
        "- Docker in 5 Minutes\n",
        "- I asked 100 millionaires how to get rich–here's what happened.\n",
        "- How I Build Projects (as an AI Engineer)\n",
        "- AI Researcher critiques Claude 3.5 sonnet\n",
        "- Data scientist explains how to predict the future\n",
        "- I bought 10 data science courses so you don’t have to\n",
        "- My AI Development Setup (From Scratch)\n",
        "- How to Build a Resume Optimizer with AI (Code Walkthrough)\n",
        "- I Quit My Job… Here’s How Much I Made 1 Year Later\n",
        "- I Was Wrong About AI Consulting (what I learned)\n",
        "\n",
        "--\n",
        "Given the YouTube video idea write 5 engaging title ideas.\n",
        "\n",
        "**Video Idea**: {idea}\n",
        "\n",
        "**Additional Guidance**:\n",
        "- Titles should be between 30 and 75 characters long\n",
        "- Only return the title ideas, nothing else!\n",
        "- Title ideas should be written as an ordered markdown list\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "UBo3n73RSCJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate Titles:\n",
        "\n",
        "%%time\n",
        "triplet_list = []\n",
        "for idea in idea_list:\n",
        "    # generate completion\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"Qwen/Qwen2.5-7B-Instruct-Turbo\",\n",
        "        messages=[\n",
        "            {\"role\": \"user\",\n",
        "             \"content\": template(idea)\n",
        "            },\n",
        "    ],\n",
        "        max_tokens=None,\n",
        "        temperature=0.7,\n",
        "        top_p=0.7,\n",
        "        top_k=50,\n",
        "        repetition_penalty=1,\n",
        "        stop=[\"<|im_end|>\"],\n",
        "    )\n",
        "\n",
        "    # parse completion (5 titles)\n",
        "    response_raw = response.choices[0].message.content\n",
        "    pattern = r\"^\\s*(?:[-*]|\\d+\\.)\\s+(.+)$\"\n",
        "    title_list = re.findall(pattern, response_raw, re.MULTILINE)\n",
        "\n",
        "    # generate all possible unique pairs\n",
        "    title_pair_list = list(combinations(title_list, 2))\n",
        "\n",
        "    # store all unique idea-title pairs in a list of dicts\n",
        "    for a,b in title_pair_list:\n",
        "        triplet_list.append({\"idea\":idea, \"title_a\": a, \"title_b\": b})"
      ],
      "metadata": {
        "id": "fi6Xxe-KSJCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Write Titles into a CSV:\n",
        "\n",
        "with open(\"data/idea-title_pairs.csv\", mode=\"w\", newline=\"\", encoding=\"utf-8\") as file:\n",
        "\n",
        "    # Extract field names from the first dictionary\n",
        "    fieldnames = triplet_list[0].keys()\n",
        "\n",
        "    # Create DictWriter object\n",
        "    writer = csv.DictWriter(file, fieldnames=fieldnames)\n",
        "\n",
        "    # Write the header row\n",
        "    writer.writeheader()\n",
        "\n",
        "    # Write data rows\n",
        "    writer.writerows(triplet_list)"
      ],
      "metadata": {
        "id": "Mb-yRpIrSSql"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-2 ( Prepare fine-tuning data )"
      ],
      "metadata": {
        "id": "e7qTGkRUSc5F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import DatasetDict, Dataset"
      ],
      "metadata": {
        "id": "L2j7X9ZYSjh1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('data/idea-title_pairs-preferences.csv')"
      ],
      "metadata": {
        "id": "F7ZUCIIhTRxt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Prompt\n",
        "\n",
        "template = lambda idea : f\"\"\"Given the YouTube video idea write an engaging title.\n",
        "\n",
        "**Video Idea**: {idea}\n",
        "\n",
        "**Additional Guidance**:\n",
        "- Title should be between 30 and 75 characters long\n",
        "- Only return the title idea, nothing else!\"\"\""
      ],
      "metadata": {
        "id": "hpDotWEEUZ3u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def idea_to_prompt(idea):\n",
        "    return [{\"role\": \"user\", \"content\": template(idea.lower())}]"
      ],
      "metadata": {
        "id": "q049UBArUbs9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['prompt'] = df['idea'].apply(idea_to_prompt)"
      ],
      "metadata": {
        "id": "KbsDgt_qUeeG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create chosen and rejected responses\n",
        "def title_to_completion(title):\n",
        "    return [{\"role\": \"assistant\", \"content\": title}]\n",
        "\n",
        "# create chosen and rejected columns\n",
        "df['chosen'] = np.where(df['title_b_preferred'] == 1, df['title_b'].apply(title_to_completion), df['title_a'].apply(title_to_completion))\n",
        "df['rejected'] = np.where(df['title_b_preferred'] == 1, df['title_a'].apply(title_to_completion), df['title_b'].apply(title_to_completion))\n",
        "\n",
        "# NOW DF IS: PROMPT = CHOOSEN = REJECTED  => # columns"
      ],
      "metadata": {
        "id": "0i2Xg19rUgjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# write data to file\n",
        "df.to_csv('data/preferences.csv')"
      ],
      "metadata": {
        "id": "oBIp8nGGUyy_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN AND TEST SPLIT\n",
        "\n",
        "# shuffle dataframe\n",
        "df_shuffled = df.iloc[:,-3:].sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# 90-10 split\n",
        "train_size = int(0.9 * len(df_shuffled))\n",
        "\n",
        "# slice accordingly\n",
        "df_train = df_shuffled.iloc[:train_size]\n",
        "df_valid = df_shuffled.iloc[train_size:]\n",
        "# Convert the pandas DataFrames back to Hugging Face Datasets\n",
        "train_ds = Dataset.from_pandas(df_train)\n",
        "valid_ds = Dataset.from_pandas(df_valid)\n",
        "\n",
        "# Combine into a DatasetDict\n",
        "dataset_dict = DatasetDict({\n",
        "    'train': train_ds,\n",
        "    'valid': valid_ds,\n",
        "})\n",
        "\n",
        "# push data to hub\n",
        "dataset_dict.push_to_hub(\"your hugging-face hub id\")"
      ],
      "metadata": {
        "id": "nFmr2KG2U6AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-3 (Finetuning The Model)"
      ],
      "metadata": {
        "id": "s5mFWlNQVET9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from trl import DPOConfig, DPOTrainer\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "import torch"
      ],
      "metadata": {
        "id": "krVtlWiYVPmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load data\n",
        "dataset = load_dataset(\"dataset address/ hf hub id for dataset\")"
      ],
      "metadata": {
        "id": "FbV95WvVVV-1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load model\n",
        "\n",
        "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token # set pad token"
      ],
      "metadata": {
        "id": "ETBnkZC-Vfnt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate title with base model\n",
        "\n",
        "def format_chat_prompt(user_input, system_message=\"You are a helpful assistant.\"):\n",
        "    \"\"\"\n",
        "    Formats user input into the chat template format with <|im_start|> and <|im_end|> tags.\n",
        "\n",
        "    Args:\n",
        "        user_input (str): The input text from the user.\n",
        "\n",
        "    Returns:\n",
        "        str: Formatted prompt for the model.\n",
        "    \"\"\"\n",
        "\n",
        "    # Format user message\n",
        "    user_prompt = f\"<|im_start|>user\\n{user_input}<|im_end|>\\n\"\n",
        "\n",
        "    # Start assistant's turn\n",
        "    assistant_prompt = \"<|im_start|>assistant\\n\"\n",
        "\n",
        "    # Combine prompts\n",
        "    formatted_prompt = user_prompt + assistant_prompt\n",
        "\n",
        "    return formatted_prompt"
      ],
      "metadata": {
        "id": "rVTFGD3OVZL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, device='mps')\n",
        "\n",
        "# Example prompt\n",
        "prompt = format_chat_prompt(dataset['valid']['prompt'][0][0]['content'])\n",
        "\n",
        "# Generate output\n",
        "outputs = generator(prompt, max_length=100, truncation=True, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "5syGRRUUVoGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train model\n",
        "\n",
        "ft_model_name = model_name.split('/')[1].replace(\"Instruct\", \"DPO\")\n",
        "\n",
        "training_args = DPOConfig(\n",
        "    output_dir=ft_model_name,\n",
        "    logging_steps=25,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3, # 3 epoch are also good if fine-tuning is only slighly changing behaviour\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"eval_loss\",\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    eval_steps=1,\n",
        ")\n",
        "\n",
        "device = torch.device('mps')"
      ],
      "metadata": {
        "id": "8v13jRWWVuIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = DPOTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    processing_class=tokenizer,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['valid'],\n",
        ")\n",
        "\n",
        "trainer.train() # this line starts actual fine-tuning"
      ],
      "metadata": {
        "id": "2cVsux-aVykk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use fine-tuned model\n",
        "\n",
        "# Load the fine-tuned model\n",
        "ft_model = trainer.model\n",
        "\n",
        "# Set up text generation pipeline\n",
        "generator = pipeline(\"text-generation\", model=ft_model, tokenizer=tokenizer, device='mps')\n",
        "\n",
        "# Example prompt\n",
        "prompt = format_chat_prompt(dataset['valid']['prompt'][0][0]['content'])\n",
        "\n",
        "# Generate output\n",
        "outputs = generator(prompt, max_length=100, truncation=True, num_return_sequences=1, temperature=0.7)\n",
        "\n",
        "print(outputs[0]['generated_text'])"
      ],
      "metadata": {
        "id": "uUBIjNpDWGjl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# push to HF hub\n",
        "\n",
        "model_id = f\"yaswanth/{ft_model_name}\"\n",
        "trainer.push_to_hub(model_id)"
      ],
      "metadata": {
        "id": "h4IFiStVWLyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step-4 (Evaluate Fine-tuned model) = by generating some titles with normal and ft-model and manully evaluating"
      ],
      "metadata": {
        "id": "r73oNsBkWTBt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import random\n",
        "from functions import generate_title\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import pandas as pd\n",
        "\n",
        "from openai import OpenAI\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "\n",
        "# load vars from .env\n",
        "load_dotenv()\n",
        "\n",
        "# connect to openai API\n",
        "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))"
      ],
      "metadata": {
        "id": "q6pvFO1mWhUk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load ideas\n",
        "\n",
        "# Open and read the CSV file\n",
        "with open('data/ideas.csv', mode='r', encoding='utf-8') as file:\n",
        "    reader = csv.reader(file)\n",
        "    next(reader) # skip first line\n",
        "\n",
        "    # intialize list to store ideas\n",
        "    idea_list = []\n",
        "\n",
        "    for row in reader:\n",
        "        idea_list.append(row[0])"
      ],
      "metadata": {
        "id": "4Qd1aT0IWlxV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Randomly select 10 ideas\n",
        "\n",
        "random.seed(0)\n",
        "random_ideas = random.sample(idea_list, 50)\n",
        "print(random_ideas)"
      ],
      "metadata": {
        "id": "Qx7sWIbYWpPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate titles from base and fine-tuned models\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2-0.5B-Instruct\")\n",
        "\n",
        "checkpoint = 258\n",
        "ft_model = AutoModelForCausalLM.from_pretrained(f\"./Qwen2-0.5B-DPO/checkpoint-{checkpoint}\")"
      ],
      "metadata": {
        "id": "na3czZrrWuPl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_title_list = []\n",
        "ft_title_list = []\n",
        "\n",
        "for idea in random_ideas:\n",
        "    base_title_list.extend(generate_title(idea, model, tokenizer, num_titles=1))\n",
        "    ft_title_list.extend(generate_title(idea, ft_model, tokenizer, num_titles=1))"
      ],
      "metadata": {
        "id": "mPET9_CuWycN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame({\"base_title\":base_title_list, \"ft_title\":ft_title_list})"
      ],
      "metadata": {
        "id": "RA2eiy2pW6Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the Outputs Manually\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Jm8wm2JeW7PV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}